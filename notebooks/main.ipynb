{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kvasir-HealthAI End-to-End Segmentation Walkthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook summarizes the preprocessing, training artefacts, inference, and explainability workflow for the U-Net based polyp segmentation pipeline. Comments stay concise while keeping each major processing stage visible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio import v2 as imageio\n",
        "from skimage import color, exposure, morphology\n",
        "from skimage.restoration import inpaint_biharmonic, denoise_bilateral\n",
        "from scipy import ndimage\n",
        "import torch\n",
        "\n",
        "from src.models.unet import UNet\n",
        "from src.training.losses import BCEDiceLoss\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 4)\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "\n",
        "REPO_ROOT = Path.cwd().resolve()\n",
        "if not (REPO_ROOT / 'notebooks').exists():\n",
        "    REPO_ROOT = REPO_ROOT.parent\n",
        "ASSET_ROOT = REPO_ROOT / 'notebooks' / 'assets'\n",
        "RESULT_ROOT = ASSET_ROOT / 'results'\n",
        "SAMPLE_ROOT = REPO_ROOT / 'sample'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_specular_highlights(rgb, value_thr=0.85, saturation_thr=0.25):\n",
        "    float_img = rgb.astype(np.float32) / 255.0\n",
        "    hsv = color.rgb2hsv(float_img)\n",
        "    bright = hsv[..., 2] > value_thr\n",
        "    low_sat = hsv[..., 1] < saturation_thr\n",
        "    mask = bright & low_sat\n",
        "    repaired = inpaint_biharmonic(float_img, mask, channel_axis=-1)\n",
        "    repaired = np.clip(repaired, 0.0, 1.0)\n",
        "    return (repaired * 255).astype(np.uint8), (mask.astype(np.uint8) * 255)\n",
        "\n",
        "def homomorphic_filter_channel(channel, cutoff=30.0, gamma_l=0.9, gamma_h=1.1, blend=0.85):\n",
        "    g0 = channel.astype(np.float32) / 255.0 + 1e-6\n",
        "    log_g = np.log(g0)\n",
        "    dft = np.fft.fft2(log_g)\n",
        "    dft_shift = np.fft.fftshift(dft)\n",
        "    h, w = channel.shape\n",
        "    yy, xx = np.ogrid[:h, :w]\n",
        "    cy, cx = h // 2, w // 2\n",
        "    dist = (xx - cx) ** 2 + (yy - cy) ** 2\n",
        "    H = 1.0 - np.exp(-(dist / (2.0 * (cutoff ** 2))))\n",
        "    H = (gamma_h - gamma_l) * H + gamma_l\n",
        "    rec = np.fft.ifft2(np.fft.ifftshift(dft_shift * H)).real\n",
        "    out = np.exp(rec)\n",
        "    out = np.clip(out, 0, None)\n",
        "    out *= (g0.mean() / (out.mean() + 1e-6))\n",
        "    lo = np.percentile(out, 3)\n",
        "    hi = np.percentile(out, 97)\n",
        "    if hi - lo > 1e-6:\n",
        "        out = np.clip((out - lo) / (hi - lo), 0, 1)\n",
        "    out = blend * out + (1.0 - blend) * g0\n",
        "    return (np.clip(out, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "def guided_smooth(channel, sigma_color=0.05, sigma_spatial=4):\n",
        "    ch = channel.astype(np.float32) / 255.0\n",
        "    filt = denoise_bilateral(ch, sigma_color=sigma_color, sigma_spatial=sigma_spatial, channel_axis=None)\n",
        "    return (np.clip(filt, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "def apply_clahe_channel(channel, clip_limit=0.03):\n",
        "    cl = exposure.equalize_adapthist(channel.astype(np.float32) / 255.0, clip_limit=clip_limit)\n",
        "    return (np.clip(cl, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "def retone_channel(channel, target_mean=145.0, target_std=80.0):\n",
        "    v = channel.astype(np.float32)\n",
        "    lo = np.percentile(v, 2)\n",
        "    hi = np.percentile(v, 98)\n",
        "    if hi - lo > 1e-6:\n",
        "        v = np.clip((v - lo) / (hi - lo), 0, 1)\n",
        "    else:\n",
        "        v = np.clip(v / 255.0, 0, 1)\n",
        "    mu = float(v.mean())\n",
        "    sd = float(v.std()) + 1e-6\n",
        "    a = (target_std / 255.0) / sd\n",
        "    b = (target_mean / 255.0) - a * mu\n",
        "    v = np.clip(a * v + b, 0, 1)\n",
        "    return (v * 255).astype(np.uint8)\n",
        "\n",
        "def apply_on_value(rgb, op):\n",
        "    hsv = color.rgb2hsv(rgb.astype(np.float32) / 255.0)\n",
        "    h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
        "    v_u8 = (v * 255).astype(np.uint8)\n",
        "    v_new = op(v_u8)\n",
        "    hsv_new = np.stack([h, s, v_new.astype(np.float32) / 255.0], axis=-1)\n",
        "    rgb_new = (color.hsv2rgb(hsv_new) * 255).astype(np.uint8)\n",
        "    return rgb_new, v_u8, v_new\n",
        "\n",
        "def overlay_mask(rgb, mask, color=(255, 0, 0), alpha=0.55):\n",
        "    out = rgb.copy().astype(np.float32)\n",
        "    mask_bool = mask.astype(bool)\n",
        "    color_arr = np.array(color, dtype=np.float32)\n",
        "    out[mask_bool] = alpha * color_arr + (1.0 - alpha) * out[mask_bool]\n",
        "    return out.astype(np.uint8)\n",
        "\n",
        "def horizontal_flip(rgb, mask):\n",
        "    return np.flip(rgb, axis=1).copy(), np.flip(mask, axis=1).copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raw colonoscopy frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_path = SAMPLE_ROOT / 'preprocess_before.png'\n",
        "assert raw_path.exists(), 'Sample image is missing.'\n",
        "raw_rgba = imageio.imread(raw_path)\n",
        "raw_rgb = raw_rgba[..., :3]\n",
        "if raw_rgb.dtype != np.uint8:\n",
        "    raw_rgb = (raw_rgb * 255).astype(np.uint8)\n",
        "\n",
        "hsv = color.rgb2hsv(raw_rgb.astype(np.float32) / 255.0)\n",
        "fallback_mask = (hsv[..., 0] < 0.08) & (hsv[..., 1] > 0.35) & (hsv[..., 2] > 0.30)\n",
        "fallback_mask = morphology.remove_small_objects(fallback_mask, 64)\n",
        "fallback_mask = morphology.remove_small_holes(fallback_mask, 64)\n",
        "fallback_mask_u8 = (fallback_mask.astype(np.uint8) * 255)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].imshow(raw_rgb)\n",
        "axes[0].set_title('Raw RGB frame')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(overlay_mask(raw_rgb, fallback_mask))\n",
        "axes[1].set_title('Heuristic polyp region (for demo)')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1 \u2013 Specular highlight suppression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spec_rgb, spec_mask = remove_specular_highlights(raw_rgb)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "axes[0].imshow(raw_rgb)\n",
        "axes[0].set_title('Before inpainting')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(spec_mask, cmap='gray')\n",
        "axes[1].set_title('Detected highlights')\n",
        "axes[1].axis('off')\n",
        "axes[2].imshow(spec_rgb)\n",
        "axes[2].set_title('After inpainting')\n",
        "axes[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2 \u2013 Homomorphic illumination correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "homo_rgb, v_before, v_after = apply_on_value(spec_rgb, homomorphic_filter_channel)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "axes[0].imshow(spec_rgb)\n",
        "axes[0].set_title('Input to homomorphic filter')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(v_before, cmap='gray')\n",
        "axes[1].set_title('Value channel before')\n",
        "axes[1].axis('off')\n",
        "axes[2].imshow(homo_rgb)\n",
        "axes[2].set_title('Value corrected output')\n",
        "axes[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3 \u2013 Guided smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guided_rgb, _, guided_v = apply_on_value(homo_rgb, guided_smooth)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].imshow(homo_rgb)\n",
        "axes[0].set_title('Before guided filter')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(guided_rgb)\n",
        "axes[1].set_title('Edge-preserving smoothing')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 4 \u2013 Local contrast boosting (CLAHE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clahe_rgb, _, clahe_v = apply_on_value(guided_rgb, apply_clahe_channel)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].imshow(guided_rgb)\n",
        "axes[0].set_title('Before CLAHE')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(clahe_rgb)\n",
        "axes[1].set_title('After CLAHE')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 5 \u2013 Retoning for consistent brightness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retone_rgb, _, retone_v = apply_on_value(clahe_rgb, retone_channel)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].imshow(clahe_rgb)\n",
        "axes[0].set_title('Before retone')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(retone_rgb)\n",
        "axes[1].set_title('After retone')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].imshow(raw_rgb)\n",
        "axes[0].set_title('Original frame')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(retone_rgb)\n",
        "axes[1].set_title('Final preprocessed frame')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset-style augmentation check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aug_rgb, aug_mask = horizontal_flip(retone_rgb, fallback_mask_u8)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].imshow(overlay_mask(retone_rgb, fallback_mask_u8))\n",
        "axes[0].set_title('Preprocessed + mask')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(overlay_mask(aug_rgb, aug_mask))\n",
        "axes[1].set_title('Horizontal flip (train mode)')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training history (pre-computed run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stored training artefacts are reused here; the notebook does not rerun optimisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_path = RESULT_ROOT / 'unet' / 'history.json'\n",
        "summary_path = RESULT_ROOT / 'unet' / 'summary.json'\n",
        "loss_curve_path = RESULT_ROOT / 'unet' / 'loss_curves.png'\n",
        "metric_curve_path = RESULT_ROOT / 'unet' / 'metric_curves.png'\n",
        "\n",
        "with open(history_path) as f:\n",
        "    history = json.load(f)\n",
        "with open(summary_path) as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "history_df = pd.DataFrame(history)\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(history_df.head())\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
        "ax[0].plot(history_df['epoch'], history_df['train_loss'], label='train')\n",
        "ax[0].plot(history_df['epoch'], history_df['val_loss'], label='val')\n",
        "ax[0].set_title('BCE+Dice loss')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].legend()\n",
        "ax[1].plot(history_df['epoch'], history_df['dice'], label='Dice')\n",
        "ax[1].plot(history_df['epoch'], history_df['iou'], label='IoU')\n",
        "ax[1].set_title('Segmentation metrics')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_img = imageio.imread(loss_curve_path)\n",
        "metric_img = imageio.imread(metric_curve_path)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].imshow(loss_img)\n",
        "axes[0].set_title('Saved loss curves')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(metric_img)\n",
        "axes[1].set_title('Saved metric curves')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('These PNGs come from the previous training session; the notebook only reads them back.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss function snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_fn = BCEDiceLoss(bce_weight=0.5)\n",
        "print(loss_fn.__doc__)\n",
        "logits = torch.randn(1, 1, 4, 4)\n",
        "targets = torch.randint(0, 2, (1, 1, 4, 4)).float()\n",
        "print(f'Example loss value: {loss_fn(logits, targets).item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model instantiation and checkpoint handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UNet(in_ch=3, out_ch=1, base=32).to(device)\n",
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "print(f'U-Net parameters: {param_count:,}')\n",
        "\n",
        "checkpoint_path = RESULT_ROOT / 'unet' / 'best_unet.pt'\n",
        "model_loaded = False\n",
        "if checkpoint_path.exists():\n",
        "    state = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "    model_loaded = True\n",
        "    print(f'Loaded checkpoint: {checkpoint_path}')\n",
        "else:\n",
        "    print('Checkpoint not found in this repository snapshot. The demo keeps the randomly initialised weights.')\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference demo on the sample frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the checkpoint file is absent, the visual output falls back to the heuristic mask so the demonstration remains interpretable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proc_size = 256\n",
        "zoom_y = proc_size / retone_rgb.shape[0]\n",
        "zoom_x = proc_size / retone_rgb.shape[1]\n",
        "resized = ndimage.zoom(retone_rgb, (zoom_y, zoom_x, 1), order=1)\n",
        "input_tensor = torch.from_numpy(resized.astype(np.float32) / 255.0).permute(2, 0, 1)[None].to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "    probs = torch.sigmoid(logits)[0, 0].cpu().numpy()\n",
        "pred_mask = (probs > 0.5).astype(np.uint8)\n",
        "pred_mask = ndimage.zoom(pred_mask, (retone_rgb.shape[0] / proc_size, retone_rgb.shape[1] / proc_size), order=0)\n",
        "if not model_loaded:\n",
        "    print('Checkpoint missing: overlay uses the heuristic mask so that the visualisation stays meaningful.')\n",
        "    pred_mask = fallback_mask.astype(np.uint8)\n",
        "overlay_pred = overlay_mask(raw_rgb, (pred_mask * 255).astype(np.uint8))\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "axes[0].imshow(raw_rgb)\n",
        "axes[0].set_title('Original frame')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(retone_rgb)\n",
        "axes[1].set_title('Preprocessed input')\n",
        "axes[1].axis('off')\n",
        "axes[2].imshow(overlay_pred)\n",
        "axes[2].set_title('Prediction overlay')\n",
        "axes[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explainability artefacts (pre-computed Grad-CAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xai_dir = RESULT_ROOT / 'xai_visualizations'\n",
        "cam_sets = {}\n",
        "for path in xai_dir.glob('unet_cam_*_input.png'):\n",
        "    key = path.stem.replace('unet_cam_', '').replace('_input', '')\n",
        "    cam_sets[key] = {'input': path}\n",
        "for path in xai_dir.glob('unet_cam_*_cam.png'):\n",
        "    key = path.stem.replace('unet_cam_', '').replace('_cam', '')\n",
        "    cam_sets.setdefault(key, {})['cam'] = path\n",
        "for path in xai_dir.glob('unet_cam_*_overlay.png'):\n",
        "    key = path.stem.replace('unet_cam_', '').replace('_overlay', '')\n",
        "    cam_sets.setdefault(key, {})['overlay'] = path\n",
        "keys = sorted(cam_sets.keys())[:3]\n",
        "fig, axes = plt.subplots(len(keys), 3, figsize=(12, 4 * len(keys)))\n",
        "if len(keys) == 1:\n",
        "    axes = axes[None, :]\n",
        "for row, key in enumerate(keys):\n",
        "    trio = cam_sets[key]\n",
        "    axes[row, 0].imshow(imageio.imread(trio['input']))\n",
        "    axes[row, 0].set_title(f'Input #{key}')\n",
        "    axes[row, 0].axis('off')\n",
        "    axes[row, 1].imshow(imageio.imread(trio['cam']))\n",
        "    axes[row, 1].set_title('Grad-CAM heatmap')\n",
        "    axes[row, 1].axis('off')\n",
        "    axes[row, 2].imshow(imageio.imread(trio['overlay']))\n",
        "    axes[row, 2].set_title('Heatmap overlay')\n",
        "    axes[row, 2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('These Grad-CAM renders were generated offline with the trained checkpoint and stored under notebooks/assets/results/xai_visualizations.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}